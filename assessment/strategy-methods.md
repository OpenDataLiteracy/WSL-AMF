## Planning Assessment Strategy

What others have done:

Use Kubler et al. (2018) metadata assessment framework? 
https://github.com/OpenDataLiteracy/WSL-AMF/blob/master/assessment/Kubler-et-al-2018.JPG

> Existence, conformance, retrievability, accuracy, open data

Or Umbrich et al. (2015) portal assessment? https://github.com/OpenDataLiteracy/WSL-AMF/blob/master/assessment/Umbrich-et-al-2015.JPG

> Retrievability, usage, completeness, accuracy, openness and contactability.  

The European Open Data Portal Monitor assesses metadata completeness by these variables:
    
> licence, author, organisation, date released and date updated 

There is also the open government benchmark proposed by [Veljkovi´c et al. (2014)](https://www.sciencedirect.com/science/article/pii/S0740624X14000434)

Also think about the [5-stars](https://www.europeandataportal.eu/elearning/en/module10/#/id/co-01)

### Portal Assessment

The complete catalog, or just the datasets, can be assessed for the following:

- Metadata completeness
- Download statistics by publisher
- Downloads versus Number of views
- Number of links to external sites for accessing data
- Summary statistics for category, downloads, agencies

- Available elements in Socrata: License, Categories, Description, Data Provided By, Row Label, Keywords, Source Link, Contact Email, Period of Time, Posting Frequency, Originator, Metadata Language

- Core Elements outside the automatic or required ones in Socrata: Description, Category, Provided By, License


### Dataset Level Assessment

A sample of datasets will be used to assess more granular aspects such as aboutness and clarity in description.

Aiming for 25% or ~125 datasets as a sample.  I will use a stratified random sample design:  The list of datasets will be divided into three levels based on the number of downloads and ~40 datasets from each level (distributed across 4 two-year groups) will be randomly selected and evaluated based on the variables listed below.

### Dataset Variables

Seems like these would be good to look at:

- File format
- Publishing Agency
- Number of downloads
- Date of publication
- Date updated
- Date metadata updated
- Percent column descriptions complete
- Is this a one year dataset that is part of ongoing data collection?
- Is the dataset description rich but understandable?


### Sample of datasets for assessment

Took 115 datasets from 12 groupings: 

Number of Downloads (<100, >101 but <1500, >1500)

Year Created (2012/2013, 2014/2015, 2016,2017, 2018,2019)

Assessment Elements:

- Title Understandable (1-No, 2-Yes)
- Description Understandable (0-absent, 1-No, 2-Yes)
- Each Row is… (0=none, 1=not understandable, or partial, 2=yes, understandable)
- Open Data Star Level
- DataDictionary (0=none, 1=not understandable, or partial, 2=yes, understandable)
- Spatial (0=none, 1=there but unclear, 2=all good)
- Temporal (0=none, 1=there but unclear, 2=all good)
- Dataset Retreivable (0=no, 1=yes)
- Resource Retreivable (0=no, 1=yes)
- Data Understandable (Are the values in the cell understandable without a dictionary?=2 (money, counts, county names, etc) Ignore missing contextual metadata (e.g. okay if you don't know what the money value is for). 1=no, 0=missing)
- Cell Accuracy  (0=more than 10 col have acc issues, 1=4-10 col, 2=1-3 col, 3=all accurate)
- Curation Needs  (0=more than 10 col need curation, 1=4-10 col, 2=1-3 col, 3=None needed)
- Currancy/usefulness (0=old/needs update, 1 = old/still useful (like data about an event), 2= uptodate/useful) 
- Belongs on Portal? 1=yes, 0=no
- Curation Notes



### Questions
